{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        print(shapes)\n",
    "        print(count)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            print(i)\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.imshow(mask)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('triangle', (179, 56, 41), (74, 99, 21)), ('circle', (57, 187, 218), (101, 69, 31))]\n",
      "2\n",
      "hs\n",
      "3951\n",
      "[False  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1008x360 with 5 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 1)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    print('hs')\n",
    "    print(np.sum(mask))\n",
    "    print(np.unique(mask))\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}