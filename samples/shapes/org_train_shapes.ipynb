{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kuki\\PycharmProjects\\Mask_RCNN\\mrcnn\\model.py:29: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kuki\\PycharmProjects\\Mask_RCNN\\mrcnn\\model.py:32: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n",
      "keras version 2.2.5\n",
      "tf version 1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        print('shapes', shapes)\n",
    "        print('count',count)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            print(i)\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.imshow(mask)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes [('triangle', (7, 212, 94), (21, 41, 31)), ('circle', (248, 75, 228), (25, 100, 27)), ('triangle', (252, 245, 39), (46, 64, 24))]\n",
      "count 3\n",
      "0\n",
      "1\n",
      "2\n",
      "[3 2 3]\n",
      "shapes [('triangle', (41, 61, 146), (27, 97, 20)), ('triangle', (67, 238, 38), (70, 32, 32)), ('square', (155, 63, 173), (26, 42, 32))]\n",
      "count 3\n",
      "0\n",
      "1\n",
      "2\n",
      "[3 3 1]\n",
      "shapes [('square', (185, 11, 112), (44, 89, 31)), ('triangle', (93, 120, 118), (80, 84, 20)), ('square', (97, 71, 120), (77, 29, 22))]\n",
      "count 3\n",
      "0\n",
      "1\n",
      "2\n",
      "[1 3 1]\n",
      "shapes [('square', (98, 211, 139), (53, 27, 24))]\n",
      "count 1\n",
      "0\n",
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1008x360 with 5 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1008x360 with 5 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1008x360 with 5 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1008x360 with 5 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    print(class_ids)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras version 2.2.5\n",
      "tf version 1.15.0\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "keras version 2.2.5\n",
      "tf version 1.15.0\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\kuki\\PycharmProjects\\Mask_RCNN\\mrcnn\\model.py:559: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kuki\\PycharmProjects\\Mask_RCNN\\mrcnn\\utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kuki\\PycharmProjects\\Mask_RCNN\\mrcnn\\model.py:606: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: C:\\Users\\kuki\\PycharmProjects\\Mask_RCNN\\logs\\shapes20200519T0029\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "keras version 2.2.5\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/1\n",
      "shapes [('square', (151, 30, 73), (94, 46, 21))]\n",
      "count 1\n",
      "0\n",
      "shapes [('circle', (103, 103, 78), (52, 37, 23)), ('triangle', (135, 158, 131), (58, 90, 28)), ('square', (84, 116, 222), (92, 102, 20))]\n",
      "count 3\n",
      "0\n",
      "1\n",
      "2\n",
      "shapes [('square', (128, 113, 24), (57, 21, 21))]\n",
      "count 1\n",
      "0\n",
      "shapes [('triangle', (164, 67, 194), (78, 79, 27)), ('circle', (124, 170, 24), (107, 93, 22))]\n",
      "count 2\n",
      "0\n",
      "1\n",
      "shapes [('circle', (223, 214, 120), (95, 21, 31)), ('circle', (91, 52, 117), (61, 34, 20))]\n",
      "count 2\n",
      "0\n",
      "1\n",
      "shapes [('triangle', (83, 112, 111), (71, 99, 32))]\n",
      "count 1\n",
      "0\n",
      "shapes [('circle', (152, 36, 234), (103, 98, 20))]\n",
      "count 1\n",
      "0\n",
      "shapes [('square', (19, 192, 23), (27, 73, 26))]\n",
      "count 1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "c:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[256,14,14,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node mrcnn_mask_conv2/BiasAdd-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[mul_50/_4881]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[256,14,14,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node mrcnn_mask_conv2/BiasAdd-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-9-6eebb012f182>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m             \u001B[0mlearning_rate\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLEARNING_RATE\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m             \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m             layers='heads')\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\Mask_RCNN\\mrcnn\\model.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001B[0m\n\u001B[0;32m   2389\u001B[0m             \u001B[0mmax_queue_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2390\u001B[0m             \u001B[0mworkers\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mworkers\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2391\u001B[1;33m             \u001B[0muse_multiprocessing\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2392\u001B[0m         )\n\u001B[0;32m   2393\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepoch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     89\u001B[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001B[0;32m     90\u001B[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001B[1;32m---> 91\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     92\u001B[0m         \u001B[0mwrapper\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_original_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     93\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit_generator\u001B[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001B[0m\n\u001B[0;32m   1656\u001B[0m             \u001B[0muse_multiprocessing\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0muse_multiprocessing\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1657\u001B[0m             \u001B[0mshuffle\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mshuffle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1658\u001B[1;33m             initial_epoch=initial_epoch)\n\u001B[0m\u001B[0;32m   1659\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1660\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0minterfaces\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlegacy_generator_methods_support\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\engine\\training_generator.py\u001B[0m in \u001B[0;36mfit_generator\u001B[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001B[0m\n\u001B[0;32m    213\u001B[0m                 outs = model.train_on_batch(x, y,\n\u001B[0;32m    214\u001B[0m                                             \u001B[0msample_weight\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msample_weight\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 215\u001B[1;33m                                             class_weight=class_weight)\n\u001B[0m\u001B[0;32m    216\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    217\u001B[0m                 \u001B[0mouts\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mto_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mouts\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mtrain_on_batch\u001B[1;34m(self, x, y, sample_weight, class_weight)\u001B[0m\n\u001B[0;32m   1447\u001B[0m             \u001B[0mins\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0my\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0msample_weights\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1448\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_train_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1449\u001B[1;33m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mins\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1450\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0munpack_singleton\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1451\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   2977\u001B[0m                     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_legacy_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2978\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2979\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2980\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2981\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mpy_any\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mis_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   2935\u001B[0m             \u001B[0mfetched\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_callable_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0marray_vals\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun_metadata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2936\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2937\u001B[1;33m             \u001B[0mfetched\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_callable_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0marray_vals\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2938\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mfetched\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2939\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kuki\\pycharmprojects\\tf1env\\tf1venv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1470\u001B[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001B[0;32m   1471\u001B[0m                                                \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1472\u001B[1;33m                                                run_metadata_ptr)\n\u001B[0m\u001B[0;32m   1473\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1474\u001B[0m           \u001B[0mproto_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[256,14,14,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node mrcnn_mask_conv2/BiasAdd-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[mul_50/_4881]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[256,14,14,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node mrcnn_mask_conv2/BiasAdd-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}